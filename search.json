[{"title":"Data Cleaning","url":"/blog/2025/04/07/Kaggle/data cleaning/","content":"处理缺失值（Handling Missing Values）1. 初始数据查看 (Initial Data Inspection) 函数&#x2F;操作: pd.read_csv(): 读取CSV文件，加载数据集（如NFL比赛数据）。 data.head(): 查看前五行数据，初步发现缺失值（显示为NaN或None）。 注意：加载时可能遇到混合数据类型警告（DtypeWarning），建议指定dtype或设置low_memory=False。 2. 计算缺失值数量 (Counting Missing Values) 关键函数: data.isnull().sum(): 统计每列的缺失值数量。 np.product(data.shape)：计算总共的records 然后 missing&#x2F;总 * 100 3. 分析缺失原因 (Understanding Why Data is Missing) 核心问题: 区分缺失原因是未记录（Not Recorded）还是不存在（Does Not Exist）。 示例: TimeSecs（比赛剩余秒数）缺失是因未记录，适合填充。 PenalizedTeam（受罚球队）缺失是因无罚球，可保留NaN或标记为”无”。 4. 删除缺失值 (Dropping Missing Values) 方法: 删除行: data.dropna() → 删除所有有missing value的records 删除列: data.dropna(axis=1) → 但丢失大量信息。 参数说明: axis=0 (按行删除): 关注的是哪些行包含缺失值。 axis=1 (按列删除): 关注的是哪些列包含缺失值。 结果检查: nfl_data.shape[1]查看原始列数，columns_with_na_dropped.shape[1]查看删除后列数。 5. 填充缺失值 (Filling Missing Values) 常用函数: fillna(): 填充缺失值。 填充固定值: data.fillna(0) → 所有NaN替换为0。 向后填充（Backward Fill）: fillna(method='bfill', axis=0).fillna(0) → 用后续值填充，剩余NaN填0。 适用场景: 时间序列数据（如TimeSecs）适合用前后值填充。 6. 关键注意事项 (Key Considerations) 权衡删除与填充: 删除列&#x2F;行可能丢失有用信息，填充可能引入噪声。 需结合业务逻辑（如PenalizedTeam列的处理）(例子)。 在决定如何处理包含缺失值的 PenalizedTeam 这一列时，不能简单地说”缺失值太多就删除”或者”用某种固定的值填充”。而是需要思考： PenalizedTeam 列的业务含义是什么？ 它记录的是在某个比赛事件中受到处罚的队伍。 为什么会出现缺失值？ 缺失可能是因为在那个特定的比赛事件中没有发生任何处罚。如果真是这样，那么缺失值本身就代表了一种”没有处罚”的状态，这在业务上是有意义的。 如果删除这一列会丢失什么信息？ 如果我们删除了 PenalizedTeam 列，我们就无法分析哪些队伍更常受到处罚，或者处罚对比赛结果的影响等相关问题。 如果填充缺失值会引入什么问题？ 如果我们用一个虚拟的队伍名称或者”无”来填充缺失值，这可能会在后续的分析中产生误导，因为它会错误地将没有处罚的事件也归类到某个队伍或者”无”这个类别下。 基于业务逻辑，更合理的处理方式是什么？ 不填充，保留缺失值： 如果缺失值代表”没有处罚”的实际业务含义，那么保留缺失值可能是最合适的做法。后续的分析需要考虑到这种缺失值的特殊含义。 创建新的类别： 可以创建一个新的类别（例如”No Penalty”）来显式地表示没有处罚的情况，而不是使用 NaN。但这仍然需要基于对业务的理解来判断是否合理。 与其他列结合分析： 可能会有其他列的信息能够帮助我们理解 PenalizedTeam 为空的情况。例如，是否存在一个”PenaltyType”列，当其为空时，PenalizedTeam 也为空？ 总而言之，”需结合业务逻辑” 要求我们在处理数据时，要像理解实际业务运作一样去理解数据，基于业务知识来判断哪些数据处理方法是合理且有意义的，避免盲目地应用技术手段而扭曲了数据的真实含义。 核心术语 (Key Terms) NaN: 缺失值的标记（Not a Number）。 DataFrame: Pandas中的二维数据结构（类似表格）。 Imputation: 缺失值填充技术（如均值、中位数、前后值填充）。 Axis参数: axis=0（行操作），axis=1（列操作）。","date":"2025-04-07","categories":["Kaggle系列"],"tags":["kaggle","ai"]},{"title":"Intro to AI Ethics","url":"/blog/2025/03/16/Kaggle/introduction to ai ethics/","content":"一级标题二级标题三级标题粗体文本 和 斜体文本 未完成任务 已完成任务 表格头1 表格头2 内容1 内容2 内容3 内容4 行内公式: $E&#x3D;mc^2$ 块级公式:$$F(x) &#x3D; \\int_{-\\infty}^{x} f(t) dt$$ 内嵌SVG:","date":"2025-03-16","categories":["Kaggle系列"],"tags":["kaggle","ai"]},{"title":"Panda basic","url":"/blog/2025/03/16/Kaggle/panda/","content":"(本博客基于Kaggle教材 link:https://www.kaggle.com/learn/pandas) PandaPanda 是一个强大的 Python 库，主要用于数据分析。在本人的学习中，我认为Panda与SQL有异曲同工之妙，如果你直接接触过SQL你可以把其理解为一个Python版本的SQL Panda 有两个核心概念：DataFrame 和 Series。 DataFrame：表格数据DataFrame 就像一个表格，有行和列。 列（column）：表格中竖着的一列数据，可以理解为 Excel 中的一列。 行（record &#x2F; row）：表格中横着的一行数据，可以理解为 Excel 中的一行。 创建 DataFrame 就像创建一个字典： Key：列名 Value：列的数据 例如： Series：一列数据Series 就像一个列表，只有一列数据。可以看作是 DataFrame 的一部分。 Series 的索引也可以是字符串： 读取数据Panda 可以读取 CSV 文件，CSV 文件是一种常见的表格数据格式。 index_col=0 的作用是告诉 Panda，CSV 文件中已经有一列作为索引了，不要再自动创建新的索引。 shape 属性可以查看 DataFrame 的大小（行数和列数）： head() 方法可以查看 DataFrame 的前几行数据： 保存数据Panda 可以将 DataFrame 保存为 CSV 文件： 数据选择选取数据Panda 提供了多种选取数据的方法。 原生 Python 方式可以使用 . 和 [] 来选取数据，和 Python 中访问对象属性的方式类似。 Panda 方式：loc 和 ilocPanda 提供了 loc 和 iloc 两种方法来选取数据。 loc：使用标签（label）来选取数据，例如行索引或列名。 iloc：使用数字（integer）来选取数据，例如行号或列号。 注意： loc 和 iloc 选取数据时都是先行后列，与 Python 中常见的先列后行不同。 例如： set_index() 方法可以将 DataFrame 中已有的某一列设置为新的索引： 条件选择可以使用条件表达式来选取满足条件的数据。 返回： 可以使用 loc 方法结合条件表达式来选取数据： 相当于 SQL 中的 SELECT * FROM reviews WHERE country = 'Italy'。 可以使用多个条件： | 表示 “或” (or) & 表示 “与” (and) isin() 方法可以判断一列的值是否在给定的列表中： isnull() 和 notnull() 方法可以判断一列的值是否为空： isnull()：如果元素是缺失值，则返回 True，否则返回 False。 notnull()：如果元素不是缺失值，则返回 True，否则返回 False。 赋值可以直接给 DataFrame 添加新的列，并赋值： 可以根据现有列的值计算出新的列的值： Summary Functions and Maps Summary Functions 摘要函数 describe(): 生成列的统计摘要（数值型与字符串型输出不同）。 数值列：count, mean, std, min, 25%, 50%, 75%, max. 字符串列：count, unique, top（最高频值）, freq（最高频次数）。 agg() - 多重统计在一个操作中执行多个统计聚合。 reset_index() - 重置索引将多级索引DataFrame转换为普通列。 排序函数sort_values() - 按值排序按指定列的值进行排序。 参数： by: 用于排序的列名 ascending: 排序方向（默认True升序，False降序） sort_index() - 按索引排序按行索引进行排序。 注意： 排序函数返回排序后的新DataFrame，不会修改原始数据。要修改原始数据，需要使用inplace=True参数。 数据类型和缺失值处理数据类型操作dtype 属性查看单个列的数据类型。 dtypes 属性查看DataFrame中所有列的数据类型。 astype() - 类型转换将列转换为指定的数据类型。 缺失值处理检测缺失值 pd.isnull(): 检测是否为缺失值(NaN) pd.notnull(): 检测是否不是缺失值 fillna() - 填充缺失值用指定的值替换NaN。 replace() - 替换值替换特定的非空值。 注意事项 字符串列的数据类型显示为object NaN在Pandas中总是以float64类型存储 处理缺失值的函数默认返回新的副本，使用inplace=True可以直接修改原数据 数据重命名和合并操作重命名操作rename() - 重命名列或索引 rename_axis() - 重命名轴 set_index() - 设置索引 对比 SQL 特性 Pandas set_index() MultiIndex SQL CREATE INDEX Composite Index 作用 修改DataFrame结构，设置行索引 创建独立的索引对象，优化查询 数据结构 索引是DataFrame的一部分 索引是独立的元数据 多重索引 支持多重索引(MultiIndex) 支持复合索引 更新方式 修改索引通常需要重建DataFrame 数据库自动维护索引更新 性能影响 Pandas索引修改的计算开销（如内存占用） 操作耗时（因维护索引） 主要用途 数据分析、标签选择、对齐和分组 查询性能优化 数据合并操作concat() - 数据拼接将多个DataFrame或Series对象连接在一起。 join() - 基于索引合并根据索引将两个DataFrame合并。 注意事项 rename()和set_index()默认返回新的DataFrame 使用inplace=True参数可以直接修改原始数据 合并操作前建议检查数据结构，避免出现意外结果 处理重复列名时，建议使用合适的后缀","date":"2025-03-16","categories":["Kaggle系列"],"tags":["kaggle","ai","machine learning"]},{"title":"Scalable Diffusion Models with Transformers","url":"/blog/2025/02/23/Image Generation/Scalable Diffusion Models with Transformers/","content":"Article Link:https://arxiv.org/pdf/2212.09748.pdf Abstract We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops—through increased transformer depth&#x2F;width or increased number of input tokens—consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL&#x2F;2 models outperform all prior diffusion models on the classconditional ImageNet 512 512 and 256 256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter. 我们探索了一种基于 Transformer 架构的新型扩散模型。我们训练图像的潜在扩散模型，用一个在潜在补丁上运行的 Transformer 替换了常用的 U-Net 主干网络。我们通过前向传递复杂度的视角（以 GFLOPs 衡量）分析了我们的扩散 Transformer (DiT) 的可扩展性。我们发现，具有更高 GFLOPs 的 DiT（通过增加 Transformer 的深度&#x2F;宽度或增加输入 tokens 的数量）始终具有更低的 FID。除了具有良好的可扩展性之外，我们最大的 DiT-XL&#x2F;2 模型在 classconditional ImageNet 512x512 和 256x256 基准测试中优于所有先前的扩散模型，在后者上实现了 2.27 的最先进 FID。 U-Net 架构U-Net 是一种常用于图像分割任务的深度学习架构，其核心是一个 U 形的编码-解码结构，由以下三部分组成： 编码器（Encoder）：压缩器 - 提取特征（卷积层组成，池化层） 解码器（Decoder）：解压器 - 补回特征（多个卷积块） 跳跃连接（Skip Connection）：连接编码器和解码器 复杂度说明GFLOPs（每秒十亿次浮点运算）用于衡量网络在处理数据时需要进行的运算量，包括加法、乘法等操作。 FID（Fréchet Inception Distance）是一种用于评估生成模型（如生成对抗网络GAN）生成图像质量的指标。 FID值越低，表示生成图像与真实图像越相似，质量越高","date":"2025-02-23","categories":["Image Generation"],"tags":["ai","image-generation"]},{"title":"机器学习入门","url":"/blog/2025/02/23/Kaggle/machine learning basic/","content":"(本博客内容基于Kaggle教程，感谢：https://www.kaggle.com/learn/intro-to-machine-learning) 机器学习是啥？简单说，就是让电脑从数据里学习，找到规律，然后用这些规律来做预测。 机器学习的几个步骤： 定义模型： 就像搭积木，选择用什么样的积木（算法）来搭建。 喂数据： 把数据给模型学习，让它找到数据里的规律。 做预测： 学习完后，让模型用学到的规律来预测新事物。 检验模型： 看看模型预测得准不准，好不好用。 Panda 介绍 (数据分析好帮手)Panda是Python里超常用的数据分析工具，可以帮你整理和分析数据。 数据结构 DataFrame: 像Excel表格一样，有行有列。 Series: 像一列数据，DataFrame就是由很多Series组成的。 核心功能 读取和写入数据（比如CSV、Excel文件）。 清洗和处理数据（把乱七八糟的数据整理干净）。 筛选和过滤数据（找到你想要的数据）。 统计和计算数据（算平均值、总和等等）。 基本使用 Sklearn 介绍 (机器学习工具箱)Sklearn是Python里最火的机器学习库，里面有很多现成的算法可以直接用。 核心功能 分类算法（Classification）： 比如判断邮件是不是垃圾邮件。 回归算法（Regression）： 比如预测房价。 聚类算法（Clustering）： 比如把用户分成不同的群体。 降维算法（Dimensionality Reduction）： 简化数据，去除不重要的信息。 模型选择（Model Selection）： 帮你找到最适合你的数据的算法。 主要优点 用起来简单。 文档写得很清楚。 用的人多，有问题容易找到答案。 能和NumPy、Pandas一起用，非常方便。 基本使用示例 模型检验 (看看模型靠不靠谱)模型预测出来的值和真实值肯定有差距，这个差距就是误差。 MAE (Mean Absolute Error)：平均绝对误差 就是把每个预测值的误差取绝对值，然后算个平均数。 为什么要取绝对值？因为误差有正有负，直接加起来可能会抵消掉。 MSE (Mean Squared Error)：均方误差 就是把每个预测值的误差取平方，然后算个平均数。 取平方也能避免正负抵消的问题，而且还能放大误差。 过拟合 (Overfitting) 和 欠拟合 (Underfitting) 过拟合： 模型在训练数据上表现很好，但在新的数据上表现很差。就像一个学生只会做他见过的题，考试的时候就傻眼了。 原因： 模型学得太细了，把训练数据里的噪音也当成了规律。 特点： 方差高： 模型对数据的变化很敏感。 泛化能力弱： 只能在特定数据上表现好。 模型太复杂。 欠拟合： 模型在训练数据上和新的数据上表现都不好。就像一个学生什么都没学懂，考试肯定不及格。 原因： 模型学得太粗略了，没能抓住数据里的主要规律。 特点： 模型太简单。 数据有问题。 训练时间不够。 基础模型介绍 决策树 (Decision Tree) 像一个流程图，根据不同的条件来做判断。 比如：如果房子有两个厕所，价格就高于18000；如果没有，就低于18000。 随机森林 (Random Forest) 有很多棵决策树，每棵树都用一部分数据和特征来训练。 可以避免过拟合和欠拟合，提高模型的准确性。","date":"2025-02-23","categories":["Kaggle系列"],"tags":["Machine learning","Kaggle","AI"]},{"title":"Categories","url":"/blog/categories/index.html","content":"","date":"2025-04-08"},{"title":"search","url":"/blog/search/index-1.html","content":"","date":"2025-03-09"},{"title":"about","url":"/blog/about/index.html","content":"KO Ho Tin (BlackCat)关于我我是一名工程专业的学生和设计爱好者，对工程学和设计充满热情。我热衷于学习新事物，并希望通过自己的努力在未来做出改变。目前专注于探索人工智能技术和机器学习，特别是在计算机视觉和深度学习研究领域。 专业技能 机器学习 (Machine Learning) 深度学习 (Deep Learning) 计算机视觉 (Computer Vision) Python TensorFlow PyTorch 最新项目经历 JA Student Company Program - 创业发展项目 MIT Innovation Academy - IoT与智能家居创新训练营 IEEE CIS Summer School - 量子计算智能项目 CityU AI and Hardware 特选课程 联系方式 Email: s20200057@ylmass.edu.hk GitHub: B143KC47 LinkedIn: BlackCat Instagram: b14ckc4t1337 欢迎通过以上方式与我联系，探讨技术、工程和创意设计等话题。","date":"2025-02-22"},{"title":"Tags","url":"/blog/tags/index.html","content":"","date":"2025-02-23"},{"title":"Search","url":"/blog/search/index.html","content":"","date":"2025-04-08"}]